{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Inspired by the article \"Summarizing Lengthy Articles\" by Mitesh Dewda on Medium.com"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "9f50c5d9c36b3154"
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2023-09-17T15:55:40.496686Z",
     "start_time": "2023-09-17T15:55:40.472197Z"
    }
   },
   "outputs": [],
   "source": [
    "import heapq\n",
    "import math\n",
    "import nltk\n",
    "import numpy\n",
    "from pdfminer.high_level import extract_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "# Read PDF document and extract the text from a range of pages.\n",
    "def extract_text_from_pdf(pdf_path, start_page=None, end_page=None):\n",
    "    return extract_text(pdf_path, page_numbers=range(start_page, end_page+1) if start_page and end_page else None)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T15:55:40.500332Z",
     "start_time": "2023-09-17T15:55:40.479923Z"
    }
   },
   "id": "62f3515cdfdc733"
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "# remove empty lines from text\n",
    "def remove_empty_lines(text):\n",
    "    return '\\n'.join(line for line in text.splitlines() if line.strip())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T15:55:40.515141Z",
     "start_time": "2023-09-17T15:55:40.483294Z"
    }
   },
   "id": "c75e135e664ebd34"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 1: Breaking the text into sentences.\n",
    "input_content = remove_empty_lines(extract_text_from_pdf('../data/CACM\\'18_Search-based_Program_Synthesis.pdf'))\n",
    "tokenized_sentences = nltk.sent_tokenize(input_content)\n",
    "for i in range(len(tokenized_sentences)):\n",
    "    print(i, tokenized_sentences[i])"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "1add51d9d30098e0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokenizer = nltk.RegexpTokenizer(r\"\\w+\")\n",
    "sentences_formatted = []\n",
    "for sentence in tokenized_sentences:\n",
    "    sentences_formatted.append(' '.join([word for word in tokenizer.tokenize(sentence)]))\n",
    "    \n",
    "total_words_in_content = len(word_tokenize(' '.join(sentences_formatted)))\n",
    "print(f\"total words in content {total_words_in_content}\")"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "234c8f7718f049cf"
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Step 2: Checking title similarity\n",
    "title = 'Search-based Program Synthesis'\n",
    "list_of_title_similarity = []\n",
    "for sentence in sentences_formatted:\n",
    "    list_of_title_similarity.append(len(list(set([word.lower() for word in sentence.split()]) & set([word.lower() for word in title.split()])))/total_words_in_content)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T15:55:42.305600Z",
     "start_time": "2023-09-17T15:55:42.273775Z"
    }
   },
   "id": "2c252535ee38f"
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [],
   "source": [
    "# Step 3: Calculating the term weight, means terms frequency and it's importance\n",
    "term_weight_list = []\n",
    "word_frequencies = {}\n",
    "for sentence in sentences_formatted:\n",
    "    sum_of_term_weight = 0 # sum of term weight for each sentence\n",
    "    \n",
    "    # calculate term weight for each word in each sentence\n",
    "    for word in word_tokenize(sentence):\n",
    "        if word not in word_frequencies.keys():\n",
    "            word_frequencies[word] = 1\n",
    "        else:\n",
    "            word_frequencies[word] += 1\n",
    "            \n",
    "        sum_of_term_weight += round(input_content.count(word) / total_words_in_content, 2)\n",
    "    \n",
    "    term_weight_list.append(sum_of_term_weight)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T15:55:42.643941Z",
     "start_time": "2023-09-17T15:55:42.281154Z"
    }
   },
   "id": "e8a93289deb536ba"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 4: POS tagging - identifying parts of speech of a sentence and tagging each word accordingly.\n",
    "nltk.download('universal_tagset')\n",
    "words = [word_tokenize(sentence) for sentence in sentences_formatted]\n",
    "pos_tags = [nltk.pos_tag(word, tagset=\"universal\") for word in words]\n",
    "\n",
    "# words which are nouns\n",
    "noun_word_list = []\n",
    "for sentence in pos_tags:\n",
    "    noun_in_sentence = []\n",
    "    for word in sentence:\n",
    "        if word[1] == 'NOUN':\n",
    "            noun_in_sentence.append(word[0])\n",
    "    noun_word_list.append(noun_in_sentence)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f766eefe2adfd2f"
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "# Step 5: Generating feature matrix\n",
    "feature_matrix = numpy.array([list_of_title_similarity, term_weight_list, noun_word_list], dtype=object)\n",
    "numpy.set_printoptions(suppress=True)\n",
    "\n",
    "final_matrix = feature_matrix.transpose()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T15:55:43.072810Z",
     "start_time": "2023-09-17T15:55:43.067061Z"
    }
   },
   "id": "acbac4c9c851653f"
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "outputs": [],
   "source": [
    "# Step 6: Generating dictionary of sentences their features\n",
    "sentence_feature_dict = {}\n",
    "\n",
    "for index, sentence in enumerate(sentences_formatted):\n",
    "    sentence_feature_dict.update({sentence: final_matrix[index]})\n",
    "    \n",
    "# calculating sum of features in each sentence and ranking them\n",
    "sentence_features_count = []\n",
    "sentence_rank_dict = {}\n",
    "for key, value in sentence_feature_dict.items():\n",
    "    sum_of_nums = 0.0\n",
    "    for item in value:\n",
    "        if isinstance(item, float):\n",
    "            sum_of_nums += item\n",
    "    sentence_features_count.append(sum_of_nums)\n",
    "    sentence_rank_dict.update({key: sum_of_nums})"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-17T15:55:43.078828Z",
     "start_time": "2023-09-17T15:55:43.071841Z"
    }
   },
   "id": "1aa889aada6f4099"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Step 7: Ranking sentences and pick up the top 30% sentences with the highest rank.\n",
    "sentences_count = math.ceil(len(list(sentence_rank_dict.keys())) * 0.03)\n",
    "sentence_rank_key_list = list(sentence_rank_dict.keys())\n",
    "\n",
    "def sort_key(key):\n",
    "    return sentence_rank_dict[key]\n",
    "\n",
    "top_sentences = heapq.nlargest(sentences_count, sentence_rank_key_list, key=sort_key)\n",
    "\n",
    "for sentence in top_sentences:\n",
    "    print(sentence + '\\n')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7e7b544a92debde9"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
